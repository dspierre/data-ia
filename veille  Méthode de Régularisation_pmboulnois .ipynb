{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  veille  M√©thode de R√©gularisation Lasso / Ridge / ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### M√©thode de R√©gularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nous allons √©tudier les diff√©rents modeles de r√©gularisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode Ridge : \n",
    "Ajouter une contrainte sur les coefficients lors de la mod√©lisation pour ma√Ætriser l‚Äôamplitude de leurs valeurs\n",
    "minùõΩ1,‚ãØ,ùõΩùëù$\\sum_{i=1}^{n}(yi$‚àí$\\sum{ùëó=1}^{ùëù}ùõΩùëó$$ùëß{ùëñùëó}$)$^{2}$\n",
    "Sous la contrainte $\\sum{j=1}^{p}ùõΩùëó^{2}‚â§ùúè$\n",
    "Norme L2ùõΩ($\\sum{j=1}^{p}ùõΩ^{2}{j}$\n",
    "$\\tau$($\\tau‚â• 0$) est un param√®tre √† fixer\n",
    "on parle de Shrinkage (r√©tr√©cissement) : on r√©tr√©cit les plages de valeur que peuvent prenre les param√®tres estim√©s. \n",
    "les variable xj doivent √™tre centr√© et r√©duite (zj) pour √©viter que les variable √† forte variance aient trop d'influence\n",
    "la variable cible y doit √™tre centr√©e pour √©vacuer la constante de la r√©gression (qui ne doit pas √™tre p√©nalis√©e), la cible y peut √™tre √©ventuellement r√©duite aussi : on travaille dans ce cas sur les param√®tre ùõΩj\n",
    "($\\tau$‚Üí0) ==> Œ≤j‚Üí0 : les variances des coefficients estim√©s sont nulles‚Ä¢($\\tau$‚Üí+$\\infty$) ==> Œ≤Ridge= Œ≤MCO\n",
    "\n",
    "Fonction de p√©nalit√©\n",
    "la r√©gresison ridge peut √™tre √©crite, de mani√®re tolement √©quivalente\n",
    "somme des carr√©s des r√©sidus p√©nalis√©s : \n",
    "\n",
    "minùõΩ1,‚ãØ,ùõΩùëù$\\sum_{i=1}^{n}(yi$‚àí$\\sum{ùëó=1}^{ùëù}ùõΩùëó$$ùëß{ùëñùëó}$)$^{2}$+$\\lambda$$\\sum{j=1}^{p}ùõΩ^{2}{j}$\n",
    "\n",
    "il y a une √©quivalence directe entre $\\lambda$ et $\\tau$\n",
    "$\\tau$ ==> 0 <=> $\\lambda$ ==> +$\\infty$ ==> 0 (tous), variances des coefficient nulles\n",
    "$\\tau$==>+$\\infty$ <=> $\\lambda$ ==> 0 : $ùõΩ{Ridge}$=$ùõΩ{MCO}$\n",
    "\n",
    "l'estimateur Ridge s'√©crit alort : \n",
    "^$ùõΩ{Ridge}$=(X'X+$\\lambda$$I{p})^{-1}$X'y\n",
    "\n",
    "On peut avoir une estimation m√™me de si (X'X) n'est pas inversible\n",
    "on voit bien que $\\lambda$ = 0, alors on a l'estimateur des MCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inconvenient du Lasso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantages et limites du lasso\n",
    "\n",
    "Les principaux avantages du lasso sont :\n",
    "\n",
    "    Grande dimension : le lasso fonctionne dans les cas o√π le nombre d'individus est inf√©rieur au nombre de variables ( n < p ) {\\displaystyle (n<p)} {\\displaystyle (n<p)}, si toutefois un faible nombre de ces variables a une influence sur les observations (hypoth√®se de parcimonie).\n",
    "    Cette propri√©t√© n'est pas vraie dans le cas de la r√©gression lin√©aire classique avec un risque associ√© qui augmente comme la dimension de l'espace des variables m√™me si l'hypoth√®se de parcimonie est v√©rifi√©e.\n",
    "    S√©lection parcimonieuse : le lasso permet de s√©lectionner un sous-ensemble restreint de variables (d√©pendant du param√®tre Œª {\\display style \\lambda } \\lambda ). Cette s√©lection restreinte permet souvent de mieux interpr√©ter un mod√®le (rasoir d'Ockham).\n",
    "    Consistance de la s√©lection : lorsque le vrai vecteur solution Œ≤ {\\displaystyle \\beta } \\beta est creux ( ‚Äñ Œ≤ ‚Äñ 0 = K < p ) {\\displaystyle (\\|\\beta \\|_{0}=K<p)} {\\displaystyle (\\|\\beta \\|_{0}=K<p)}, c'est-√†-dire que seul un sous-ensemble de variables est utilis√© pour la pr√©diction, sous de bonnes conditions, le lasso sera en mesure de s√©lectionner ces variables d'int√©r√™ts avant toutes autres variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantage \n",
    "Capacit√© √† s√©lectionner les variables en acceptant les coefficients nuls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconv√©nients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les probl√®mes √† tr√®s grandes dimensions (p >> n), LASSO ne\n",
    "s√©lectionne que n variables pr√©dictives au maximum, m√©caniquement.\n",
    "C‚Äôest une vraie limitation de l‚Äôalgorithme.\n",
    "Parmi un groupe de variables corr√©l√©es, LASSO en choisit une, celle qui\n",
    "est la plus li√©e √† a cible souvent, masquant l‚Äôinfluence des autres. Cet\n",
    "inconv√©nient est inh√©rent aux techniques int√©grant un m√©canisme de\n",
    "s√©lection de variables (ex. arbres de d√©cision, quoique‚Ä¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticnet ‚Äì Combiner les avantages de Ridge et Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulation sous la forme d‚Äôun probl√®me d‚Äôoptimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'utilisation de cette fonction de p√©nalit√© pr√©sente plusieurs limites.\n",
    "[1] Par exemple, dans le cas \"grand p , petit n \" (donn√©es de grande dimension avec quelques exemples), \n",
    "le LASSO s√©lectionne au plus n variables avant de saturer. De plus, s'il existe un groupe de variables \n",
    "hautement corr√©l√©es, le LASSO a tendance √† s√©lectionner une variable dans un groupe et √† ignorer les autres.\n",
    "Pour surmonter ces limites, le filet √©lastique ajoute une partie quadratique √† la p√©nalit√© \n",
    "$({\\ displaystyle \\ | \\ beta \\ | ^ {2}}\\ | \\ beta \\ | ^ 2)$, \n",
    "qui, lorsqu'elle est utilis√©e seule, est une r√©gression de cr√™te (√©galement appel√©e r√©gularisation de Tikhonov ).\n",
    "Les estimations de la m√©thode du filet √©lastique sont d√©finies par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'Elastic-net a √©t√© introduit afin de surmonter deux \"limitations\" du lasso. Premi√®rement, le lasso ne peut s√©lectionner qu'au plus {\\displaystyle n}n variables dans le cas o√π {\\displaystyle n<p}{\\displaystyle n<p}. Deuxi√®mement, en pr√©sence d'un groupe de variables fortement corr√©l√©es, le lasso ne s√©lectionne g√©n√©ralement qu'une seule variable du groupe. L'id√©e est donc \n",
    "d'ajouter au lasso une p√©nalit√© ridge. Ainsi l'objectif de l'Elastic-Net est :\n",
    "${\\displaystyle \\min _{\\beta \\in \\mathbb {R} ^{p}}$\n",
    "${\\frac {1}{2}}\\|y-X\\beta \\|_{2}^{2}+\\lambda _{1}\\|\\beta \\|_{1}+\\lambda _{2}\\|\\beta \\|_{2}^{2}}{\\displaystyle \\min _{\\beta \\in \\mathbb$ \n",
    "${R} ^{p}}{\\frac {1}{2}}\\|y-X\\beta \\|_{2}^{2}+\\lambda _{1}\\|\\beta \\|_{1}+\\lambda _{2}\\|\\beta \\|_{2}^{2}}$\n",
    "avec\n",
    "${\\displaystyle \\lambda _{1}\\geq 0}{\\displaystyle \\lambda _{1}\\geq 0} et {\\displaystyle \\lambda _{2}\\geq 0}{\\displaystyle \\lambda _{2}\\geq 0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capacit√© de s√©lection de variables du LASSO conserv√©e (coefficients nuls) :\n",
    "exclusion des variables non pertinentes\n",
    "‚Ä¢ Groupe de variables pr√©dictives corr√©l√©es, partage des poids (comme\n",
    "Ridge) et non plus s√©lection arbitraire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inconvenient de lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pr√©sence de variables explicatrices corrol√©es,\n",
    "le modele lasso en choisit une arbitrairement \n",
    "et mes les autre √† 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons √©tudier trois modeles de r√©gularisation le modele Elasticnet est la combinaison des  meilleurs √©lements  de Lasso et du modele Ridge   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
